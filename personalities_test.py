# -*- coding: utf-8 -*-
"""Personalities_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1018LwLZLFfUi2RXi4RA_3rHet4fDYxwu
"""



import pandas as pd

# Load the dataset
data = pd.read_excel("/content/Merged_data.xlsx")

# Function to fill missing values with the mean of non-missing values for each label

# Convert potential string "NA" to NaN
data.replace("NA", float("nan"), inplace=True)

# Group by 'Label(AOIs)' and fill NaN with the mean of each group
filled_data = data.copy()

# Get unique labels
labels = filled_data['Label(AOIs)'].unique()

# Iterate over each label and fill NaN values with the mean of non-NaN values for that label
for label in labels:
    label_data = filled_data[filled_data['Label(AOIs)'] == label]
    filled_data.loc[label_data.index] = label_data.fillna(label_data.mode())

# Display the filled dataset
print(filled_data)

import pandas as pd
import numpy as np

# Group by 'Label(AOIs)' and fill NaN with the mean of each group
filled_data = data.copy()

# Get unique labels
labels = filled_data['Label(AOIs)'].unique()

# Iterate over each label and fill NaN values with the mean of non-NaN values for that label
for label in labels:
    label_data = filled_data[filled_data['Label(AOIs)'] == label]
    filled_data.loc[label_data.index, ['FixationCount', 'FixationDuration', 'SaccadeDuration', 'SaccadeAmplitude']] = \
        label_data[['FixationCount', 'FixationDuration', 'SaccadeDuration', 'SaccadeAmplitude']].fillna(
            label_data[['FixationCount', 'FixationDuration', 'SaccadeDuration', 'SaccadeAmplitude']].mean()
        )

# Display the filled dataset
print(filled_data)
means = filled_data.groupby("Label(AOIs)").transform(np.mean)
df_filled = filled_data.fillna(means)





df_filled.set_index('Respondent Name',inplace=True)

df_filled['Label(AOIs)']

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the categorical column
df_filled['Label(AOIs)'] = label_encoder.fit_transform(df_filled['Label(AOIs)'])

import pandas as pd
from sklearn.impute import SimpleImputer

# Assuming 'df_filled' is your DataFrame with missing values

# Extract the index column
index_column = df_filled.index

# Initialize SimpleImputer
imputer = SimpleImputer(strategy='mean')  # You can choose a different strategy if needed

# Impute missing values
df_filled_imputed = pd.DataFrame(imputer.fit_transform(df_filled), columns=df_filled.columns, index=index_column)

df_filled_imputed

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
corr_matrix = df_filled_imputed.corr()

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# Select the relevant columns for clustering and machine learning
eye_movement_features = ['FixationCount', 'FixationDuration', 'SaccadeDuration', 'SaccadeAmplitude']
personality_traits = ['Emotionality', 'Extraversion', 'Agreeableness', 'Conscientiousness', 'Openness', 'Honesty_Humility']

from sklearn.preprocessing import StandardScaler
# Normalize the data
scaler = StandardScaler()
df_filled_imputed[eye_movement_features] = scaler.fit_transform(df_filled_imputed[eye_movement_features])

import pandas as pd

# Assuming df is your DataFrame with the provided dataset

# Define the character traits columns
character_traits = ['Emotionality', 'Extraversion', 'Agreeableness', 'Conscientiousness', 'Openness', 'Honesty_Humility']

# Function to calculate the most dominant character trait for each respondent
def calculate_dominant_personality(row):
    max_personality_trait = None
    max_score = float('-inf')
    for trait in character_traits:
        if row[trait] > max_score:
            max_score = row[trait]
            max_personality_trait = trait
    return max_personality_trait

# Apply the function to each row to find the most dominant personality trait
df_filled_imputed['Most_Dominant_Personality'] = df_filled_imputed.apply(calculate_dominant_personality, axis=1)

print(df_filled_imputed)

from sklearn.preprocessing import LabelEncoder

# Assuming label_encoder is your LabelEncoder object
label_encoder = LabelEncoder()

# Fit and transform the column 'Most_Dominant_Personality'
df_filled_imputed['Most_Dominant_Personality'] = label_encoder.fit_transform(df_filled_imputed['Most_Dominant_Personality'])

# Get the unique classes learned by the LabelEncoder
unique_classes = label_encoder.classes_

# Create a dictionary to map encoded labels to original labels
label_mapping = {label: original_label for label, original_label in enumerate(unique_classes)}

# Display the label mapping
for encoded_label, original_label in label_mapping.items():
    print(f"Encoded label {encoded_label}: Original label {original_label}")

df_filled_imputed.columns

df_filled_imputed = df_filled_imputed[[ 'FixationCount', 'FixationDuration', 'SaccadeDuration',
       'SaccadeAmplitude','Most_Dominant_Personality']]

df_filled_imputed

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Assuming df contains your dataset

# Group by respondent and aggregate the features
grouped_df = df_filled_imputed.groupby('Respondent Name').agg('mean')

# Normalize/Standardize the features
#scaler = StandardScaler()
#X_scaled = scaler.fit_transform(grouped_df)

# Define the number of clusters
num_clusters = 3

# Initialize KMeans model
kmeans = KMeans(n_clusters=num_clusters, random_state=42)

# Fit the model to the scaled data
kmeans.fit(grouped_df)

# Assign clusters to each respondent
grouped_df['Cluster'] = kmeans.labels_

# Merge the cluster information back to the original dataset
df_filled_imputed = df_filled_imputed.merge(grouped_df['Cluster'], how='left', on='Respondent Name')

grouped_df

from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Define a list of clustering algorithms to compare
clustering_algorithms = {
    'KMeans': KMeans(n_clusters=3),
    'Agglomerative': AgglomerativeClustering(n_clusters=3),
    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),
    'GMM': GaussianMixture(n_components=3)
}

# Dictionary to store evaluation results for each algorithm
evaluation_results = {}

# Iterate over each clustering algorithm
for name, algorithm in clustering_algorithms.items():
    # Fit the algorithm to the data
    algorithm.fit(grouped_df)

    # Predict cluster labels
    if hasattr(algorithm, 'labels_'):
        cluster_labels = algorithm.labels_
    else:
        cluster_labels = algorithm.predict(grouped_df)

    # Calculate evaluation metrics
    silhouette = silhouette_score(grouped_df, cluster_labels)
    davies_bouldin = davies_bouldin_score(grouped_df, cluster_labels)
    calinski_harabasz = calinski_harabasz_score(grouped_df, cluster_labels)

    # Store evaluation results
    evaluation_results[name] = {'Silhouette': silhouette, 'Davies-Bouldin': davies_bouldin, 'Calinski-Harabasz': calinski_harabasz}

# Print evaluation results
for name, metrics in evaluation_results.items():
    print(f'{name} Evaluation Metrics:')
    for metric, value in metrics.items():
        print(f'{metric}: {value}')
    print()

grouped_df

# Print the count of items in each cluster
cluster_counts = grouped_df['Cluster'].value_counts()
print("Items in each cluster:")
print(cluster_counts)

grouped_df

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, SimpleRNN, LSTM
from keras.callbacks import ModelCheckpoint
from sklearn.metrics import accuracy_score, mean_squared_error, cohen_kappa_score, confusion_matrix
from keras.models import load_model
# Split the data into features (X) and target variable (y)
X = grouped_df.drop(columns=['Cluster'])
y = grouped_df['Cluster']

# Normalize/Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.15, random_state=42)

# Callback to save the best model during training
checkpoint = ModelCheckpoint("best_model.h5", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

# Define CNN model
cnn_model = Sequential()
cnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
cnn_model.add(MaxPooling1D(pool_size=2))
cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='relu'))
cnn_model.add(Dense(3, activation='softmax'))  # Adjust output units based on number of clusters
cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Define RNN model
rnn_model = Sequential()
rnn_model.add(SimpleRNN(32, input_shape=(X_train.shape[1], 1)))
rnn_model.add(Dense(128, activation='relu'))
rnn_model.add(Dense(3, activation='softmax'))  # Adjust output units based on number of clusters
rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Define LSTM model
lstm_model = Sequential()
lstm_model.add(LSTM(32, input_shape=(X_train.shape[1], 1)))
lstm_model.add(Dense(128, activation='relu'))
lstm_model.add(Dense(3, activation='softmax'))  # Adjust output units based on number of clusters
lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the models
cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_test, y_test), callbacks=[checkpoint])
rnn_model.fit(X_train.reshape((X_train.shape[0], X_train.shape[1], 1)), y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_test.reshape((X_test.shape[0], X_test.shape[1], 1)), y_test), callbacks=[checkpoint])
lstm_model.fit(X_train.reshape((X_train.shape[0], X_train.shape[1], 1)), y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_test.reshape((X_test.shape[0], X_test.shape[1], 1)), y_test), callbacks=[checkpoint])

# Load the best model
best_model = load_model("best_model.h5")

# Make predictions using the best model
predictions = best_model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1).astype(int)

# Save the results of predictions
np.save("predicted_classes.npy", predicted_classes)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, predicted_classes)
rmse = mean_squared_error(y_test, predicted_classes, squared=False)
kappa = cohen_kappa_score(y_test, predicted_classes)
conf_matrix = confusion_matrix(y_test, predicted_classes)

print("Accuracy:", accuracy)
print("RMSE:", rmse)
print("Cohen's Kappa:", kappa)
print("Confusion Matrix:\n", conf_matrix)

import matplotlib.pyplot as plt

# Display the architecture of the best model
print("Architecture of the Best Model:")
best_model.summary()

import matplotlib.pyplot as plt

# Create a mapping between encoded labels and original labels
label_mapping = {
    0: 'Agreeableness',
    1: 'Conscientiousness',
    2: 'Emotionality',
    3: 'Extraversion',
    4: 'Honesty_Humility',
    5: 'Openness'
}

# Group by 'Cluster' and find the most common personality type in each cluster
cluster_dominant_personality = df_filled_imputed.groupby('Cluster')['Most_Dominant_Personality'].agg(lambda x: x.value_counts().idxmax())

# Map encoded labels to original labels
cluster_dominant_personality = cluster_dominant_personality.map(label_mapping)

# Calculate the number of respondents in each cluster
cluster_counts = df_filled_imputed['Cluster'].value_counts()

# Plot a pie chart
plt.figure(figsize=(8, 8))
plt.pie(cluster_counts, labels=cluster_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Respondents across Clusters')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle
plt.show()

# Display the dominant personality type in each cluster
print("Dominant Personality in Each Cluster:")
print(cluster_dominant_personality)